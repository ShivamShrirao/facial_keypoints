{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../dnn_from_scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"../Celeb100k/100k\"\n",
    "with open(DATA_DIR+\".txt\",\"r\") as f:\n",
    "    names=f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(st,btsz):\n",
    "    X_train=[]\n",
    "    for ii in names[st:st+btsz]:\n",
    "        img=imread(DATA_DIR+\"/\"+ii)\n",
    "        img=cv2.resize(img, dsize=(64,64))#, interpolation=cv2.INTER_CUBIC)\n",
    "        X_train.append(img/255*2-1)\n",
    "    return np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 487\n",
      "Seed: 761\n"
     ]
    }
   ],
   "source": [
    "from nnet.network import Sequential,layers\n",
    "from nnet.layers import conv2d,max_pool,flatten,dense,dropout,conv2d,conv2dtranspose,upsampling,reshape,BatchNormalization,Activation\n",
    "from nnet import functions\n",
    "from nnet import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽\n",
      "Layer (type)               Output Shape             Activation        Param #\n",
      "==========================================================================================\n",
      "0 input_layer(InputLayer) (None, 100)                echo             0\n",
      "__________________________________________________________________________________________\n",
      "1 dense(dense)            (None, 16384)              leakyRelu        1654784\n",
      "__________________________________________________________________________________________\n",
      "2 reshape(reshape)        (None, 8, 8, 256)          echo             0\n",
      "__________________________________________________________________________________________\n",
      "3 conv2dtranspose(conv2dt (None, 16, 16, 256)        leakyRelu        590080\n",
      "__________________________________________________________________________________________\n",
      "4 BatchNormalization(Batc (None, 16, 16, 256)        echo             1024\n",
      "__________________________________________________________________________________________\n",
      "5 conv2dtranspose(conv2dt (None, 32, 32, 128)        leakyRelu        295040\n",
      "__________________________________________________________________________________________\n",
      "6 BatchNormalization(Batc (None, 32, 32, 128)        echo             512\n",
      "__________________________________________________________________________________________\n",
      "7 conv2dtranspose(conv2dt (None, 64, 64, 64)         leakyRelu        73792\n",
      "__________________________________________________________________________________________\n",
      "8 BatchNormalization(Batc (None, 64, 64, 64)         echo             256\n",
      "__________________________________________________________________________________________\n",
      "9 conv2d(conv2d)          (None, 64, 64, 3)          tanh             1731\n",
      "==========================================================================================\n",
      "Total Params: 2,617,219\n",
      "Trainable Params: 2,616,323\n",
      "Non-trainable Params: 896\n"
     ]
    }
   ],
   "source": [
    "def generator():\n",
    "    model=Sequential()\n",
    "    model.add(dense(8*8*256,activation=functions.leakyRelu,input_shape=100))\n",
    "    model.add(reshape((8,8,256)))\n",
    "    model.add(conv2dtranspose(256,kernel_size=3,stride=[2,2],activation=functions.leakyRelu))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(conv2dtranspose(128,kernel_size=3,stride=[2,2],activation=functions.leakyRelu))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(conv2dtranspose(64,kernel_size=3,stride=[2,2],activation=functions.leakyRelu))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(conv2d(3,kernel_size=3,activation=functions.tanh))\n",
    "    return model\n",
    "g=generator()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽\n",
      "Layer (type)               Output Shape             Activation        Param #\n",
      "==========================================================================================\n",
      "0 input_layer(InputLayer) (None, 64, 64, 3)          echo             0\n",
      "__________________________________________________________________________________________\n",
      "1 conv2d(conv2d)          (None, 32, 32, 32)         leakyRelu        896\n",
      "__________________________________________________________________________________________\n",
      "2 BatchNormalization(Batc (None, 32, 32, 32)         echo             128\n",
      "__________________________________________________________________________________________\n",
      "3 dropout(dropout)        (None, 32, 32, 32)         echo             0\n",
      "__________________________________________________________________________________________\n",
      "4 conv2d(conv2d)          (None, 16, 16, 64)         leakyRelu        18496\n",
      "__________________________________________________________________________________________\n",
      "5 BatchNormalization(Batc (None, 16, 16, 64)         echo             256\n",
      "__________________________________________________________________________________________\n",
      "6 dropout(dropout)        (None, 16, 16, 64)         echo             0\n",
      "__________________________________________________________________________________________\n",
      "7 conv2d(conv2d)          (None, 8, 8, 128)          leakyRelu        73856\n",
      "__________________________________________________________________________________________\n",
      "8 BatchNormalization(Batc (None, 8, 8, 128)          echo             512\n",
      "__________________________________________________________________________________________\n",
      "9 dropout(dropout)        (None, 8, 8, 128)          echo             0\n",
      "__________________________________________________________________________________________\n",
      "10 flatten(flatten)       (None, 8192)               echo             0\n",
      "__________________________________________________________________________________________\n",
      "11 dropout(dropout)       (None, 8192)               echo             0\n",
      "__________________________________________________________________________________________\n",
      "12 dense(dense)           (None, 256)                leakyRelu        2097408\n",
      "__________________________________________________________________________________________\n",
      "13 dense(dense)           (None, 1)                  sigmoid          257\n",
      "==========================================================================================\n",
      "Total Params: 2,191,809\n",
      "Trainable Params: 2,191,361\n",
      "Non-trainable Params: 448\n"
     ]
    }
   ],
   "source": [
    "def discriminator():\n",
    "    model=Sequential()\n",
    "    model.add(conv2d(32,kernel_size=3,stride=[2,2],activation=functions.leakyRelu,input_shape=(64,64,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(dropout(0.1))\n",
    "    model.add(conv2d(64,kernel_size=3,stride=[2,2],activation=functions.leakyRelu))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(dropout(0.2))\n",
    "    model.add(conv2d(128,kernel_size=3,stride=[2,2],activation=functions.leakyRelu))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(dropout(0.25))\n",
    "    model.add(flatten())\n",
    "    model.add(dropout(0.2))\n",
    "    model.add(dense(256,activation=functions.leakyRelu))\n",
    "    model.add(dense(1,activation=functions.sigmoid))\n",
    "    return model\n",
    "d=discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.compile(optimizer=optimizers.adam,loss=functions.mean_squared_error,learning_rate=0.002)\n",
    "d.compile(optimizer=optimizers.adam,loss=functions.cross_entropy_with_logits,learning_rate=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdnoise=np.random.randn(64,100).astype(np.float32)\n",
    "gen=g.predict(svdnoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample():\n",
    "    noise=np.random.randn(1,100).astype(np.float32)\n",
    "    gen=g.predict(noise)\n",
    "    plt.imshow((gen[0]+1)/2,interpolation='bicubic')\n",
    "    plt.show()\n",
    "    print(d.predict(gen)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_grid(noise,show=True,save=False):\n",
    "    fig, ax = plt.subplots(nrows=5,ncols=5,figsize=[20,20],gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "    fig.patch.set_facecolor('white')\n",
    "    gen=g.predict(noise)\n",
    "    for i,axi in enumerate(ax.flat):\n",
    "        axi.axis(\"off\")\n",
    "        axi.imshow((gen[i].squeeze()+1)/2,interpolation='bicubic')\n",
    "    if save:\n",
    "        fig.savefig(\"timelapse/\"+str(ccu)+\"img\"+str(np.random.randn())[:5]+\".png\",bbox_inches='tight')\n",
    "    if not show:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    global ccu,svdnoise\n",
    "    batch_size=64\n",
    "    dsz=len(names)\n",
    "    # Label real ones as 90% real\n",
    "    y_fake=np.zeros((batch_size,1),dtype=np.float32)\n",
    "    y_real=np.ones((batch_size,1),dtype=np.float32)\n",
    "    y_gen=np.ones((batch_size,1),dtype=np.float32)\n",
    "    for step in range(dsz//batch_size):\n",
    "        # Get random real images\n",
    "        real_images=get_batch(ccu,batch_size)\n",
    "        ccu+=batch_size\n",
    "        if ccu>len(names):\n",
    "            ccu=0\n",
    "            break\n",
    "        stt=time()\n",
    "        noise=np.random.randn(batch_size,100).astype(np.float32)\n",
    "        # Generate fake images from noise\n",
    "        generated_images=g.predict(noise)\n",
    "        # Train discriminator\n",
    "        y_fake=np.random.uniform(0,0.05,(batch_size,1)).astype(np.float32)\n",
    "        y_real=np.random.uniform(0.9,1,(batch_size,1)).astype(np.float32)\n",
    "        dout1=d.train_on_batch(real_images,y_real)\n",
    "        dout2=d.train_on_batch(generated_images,y_fake)\n",
    "        dloss=functions.cross_entropy(logits=dout2,labels=y_fake).sum()\n",
    "        # Treat noised input of generator as real data\n",
    "#         err=np.array(0)\n",
    "        noise=np.random.randn(batch_size,100).astype(np.float32)\n",
    "        # Train generator\n",
    "        gout=g.forward(noise)\n",
    "        # do not train discriminator and find delta for generator\n",
    "        dout,err=d.not_train_on_batch(gout,y_gen)\n",
    "        # backpropogate the generator and update weights\n",
    "        g.backprop(err,g.lenseq_m1)\n",
    "        g.optimizer(g.sequence,g.learning_rate,g.beta)\n",
    "        if not step%40:\n",
    "            print(\"\\nSaving grid.\")\n",
    "            gen_grid(svdnoise[:25],show=False,save=True)\n",
    "            gen_sample()\n",
    "        print(\"\\rProgress: {:.2f} %    {}    Dloss: {}    err: {}    Step time: {:.3f}s    _\".format(step*batch_size*100/dsz,ccu,dloss,abs(err).sum(),time()-stt),end='')\n",
    "    if ccu>len(names):\n",
    "        ccu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH:\",epoch+1,'/',epochs)\n",
    "    st_tm=time()\n",
    "    run()\n",
    "    print(\"\\nEpoch time: {}:{}s\".format(int(time()-st_tm)//60,int(time()-st_tm)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Using generator with just batch size 1 for first time is causing images to be generated extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise=np.random.randn(32,100).astype(np.float32)\n",
    "gen=g.predict(noise)\n",
    "gen_sample()\n",
    "img=get_batch(np.random.randint(low=0,high=len(names)),1)\n",
    "plt.imshow((img[0]+1)/2,interpolation='bicubic')\n",
    "plt.show()\n",
    "print(d.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=np.random.randn(25,100).astype(np.float32)\n",
    "gen_grid(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.load_weights(\"generator.w8s\")\n",
    "# d.load_weights(\"discriminator.w8s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.save_weights(\"generator.w8s\")\n",
    "# d.save_weights(\"discriminator.w8s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
